


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')


df


test_df





df.info()


df.describe()


print(df.isnull().sum())
print(test_df.isnull().sum())


# df = df.drop('Cabin',axis=1)
test_df = test_df.drop('Cabin',axis=1)


df['Age']  = df['Age'].fillna(df['Age'].mean())



df.head()


df = df.drop('Name',axis=1)


df.head()


categorical = ['Survived','Pclass','Sex','SibSp','Parch','Embarked']
numerical = ['Age', 'Fare']


for cat in categorical:
    plt.figure()
    sns.countplot(data = df, x = cat, hue = 'Survived')






for num in numerical:
    plt.figure()
    sns.histplot(data=df, x = num, hue='Survived',palette='tab10', kde=True)


df[df['Fare'] > 300]


Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1

upper_bound = Q3 + 1.5 * IQR

# Replace outliers with median
df.loc[df['Fare'] > upper_bound, 'Fare'] = df['Fare'].median()



sns.histplot(data=df, x = 'Fare', hue='Survived',palette='tab10', kde=True)


# Encode categorical variables
encoded_df = pd.get_dummies(df[categorical], drop_first=True)

# Calculate correlation matrix
correlation_matrix = encoded_df.corr()

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Categorical Data')
plt.show()


#pclass and sex are most correlated with survived


encoded_df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
encoded_df['IsAlone'] = (encoded_df['FamilySize'] == 1).astype(int)
encoded_df = encoded_df.drop(['Parch','SibSp'],axis=1)


plt.figure(figsize=(10, 6))
sns.heatmap(encoded_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Categorical Data')
plt.show()


encoded_df.sample(8)


from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()
scaler.fit_transform(encoded_df)


encoded_df.sample(9)


encoded_df = encoded_df.astype(int)


encoded_df.sample(9)
X = encoded_df.drop('Survived',axis=1)
y = encoded_df['Survived']


from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)


from sklearn.linear_model import LogisticRegression


model_lr = LogisticRegression()


model_lr.fit(X, y)


test_df['Age']  = test_df['Age'].fillna(test_df['Age'].mean())



test_df = test_df.drop('Name',axis=1)



Q1 = test_df['Fare'].quantile(0.25)
Q3 = test_df['Fare'].quantile(0.75)
IQR = Q3 - Q1

upper_bound = Q3 + 1.5 * IQR

# Replace outliers with median
test_df.loc[test_df['Fare'] > upper_bound, 'Fare'] = test_df['Fare'].median()


test_df


# Encode categorical variables
passenger_id = test_df['PassengerId']
categorical = ['Pclass','Sex','SibSp','Parch','Embarked']
encoded_df_test = pd.get_dummies(test_df[categorical], drop_first=True)




encoded_df_test


encoded_df_test['FamilySize'] = df['SibSp'] + df['Parch'] + 1
encoded_df_test['IsAlone'] = (encoded_df['FamilySize'] == 1).astype(int)
# encoded_df_test = encoded_df_test.drop(['Parch','SibSp'],axis=1)


encoded_df_test = encoded_df_test.drop(['Parch','SibSp'],axis=1)


scaler = StandardScaler()
scaler.fit_transform(encoded_df_test)


encoded_df_test = encoded_df_test.astype(int)


final_pred = model_lr.predict(encoded_df_test)


import pandas as pd

submit = pd.DataFrame({
    "PassengerId": passenger_id,
    "Survived": final_pred
})

submit.to_csv("submission.csv", index=False)



submit











encoded_df_test


y_pred_lr = model_lr.predict(X_test)


from sklearn.metrics import accuracy_score


accuracy_lr = accuracy_score(y_test, y_pred_lr)


accuracy_lr


from sklearn.metrics import classification_report, confusion_matrix

print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))



pd.Series(model_lr.coef_[0], index=X.columns).sort_values()






from sklearn.tree import DecisionTreeClassifier


dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train,y_train)


y_pred_dt = dt.predict(X_test)


acc_dt_test = accuracy_score(y_pred_dt,y_test)


acc_dt_test


y_pred_dt_train = dt.predict(X_train)


acc_dt_train = accuracy_score(y_pred_dt_train,y_train)


acc_dt_train


print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))


print(dt.score(X_train, y_train))
print(dt.score(X_test, y_test))






for max_d in [2,3,5,10]:
    for min_ss in [2,10,20]:
        dt = DecisionTreeClassifier(random_state=42, max_depth=max_d, min_samples_split=min_ss)
        dt.fit(X_train,y_train)
        print("max_depth ",max_d)
        print("min_sample_split ",min_ss)
        print(dt.score(X_train, y_train))
        print(dt.score(X_test, y_test))





pd.Series(dt.feature_importances_, index=X.columns)\
  .sort_values(ascending=False)






from sklearn.tree import plot_tree
plt.figure(figsize=(20,10))
plot_tree(dt, feature_names=X.columns, class_names=['No','Yes'], filled=True)




